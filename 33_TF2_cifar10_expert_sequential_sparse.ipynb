{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "33_TF2_cifar10_expert_sequential_sparse.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PmcPzSWs-LTI"
      },
      "source": [
        "# CIFAR10\n",
        "\n",
        "아래의 code는 google colab 에서 시험된 결과이다.\n",
        "Anaconda 에서 제공되는 jupyter notebook에서는 작동이 안될수 있다.\n",
        "추천하는 방법은 notebook이 아니라 python 파일 자체를 실행하라는 것이다. TPU 예제만 빼고는 anaconda prompt 에서 *.py 파일을 실행하는 방법을 추천한다.\n",
        "compare 도구로서는 \"BCcompare\"를 추천한다. working day로 30일간 무료로 사용이 가능하다.\n",
        "무료 사용후 재 설치 하면 다시 30 working day동안 사용이 가능하다.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sMPrRugTzwzZ"
      },
      "source": [
        "Compare 도구를 사용하여, \"21_TF2_MNIST_expert_sequential_sparse.py\"와 \"33_TF2_cifar10_expert_sequential_sparse.py\"를 비교하여 차이가 나는 부분을 살펴보자.\n",
        "\n",
        "주된 차이점은 입력의 차원이다.\n",
        "\n",
        "```\n",
        "# X_train = X_train[..., tf.newaxis]\n",
        "# X_test = X_test[..., tf.newaxis]\n",
        "\n",
        "```\n",
        "MNIST의 경우에는 입력의 차원을 변경할 필요가 있으나, CIFAR10의 경우에는 이러한 작업이 필요가 없다.\n",
        "\n",
        "\n",
        "**향후에 쓰는 모든 코드들도 입력 차원에 대해서 주의하자.\n",
        "이는 초보 개발자가 주로 하는 실수들이다.**\n",
        "\n",
        "아래의 code를 colab에서 GPU 모드에서 실행 시켜 보기 바란다.\n",
        "Non-GPU 에서는 시간이 좀 많이 걸린다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VxwNCuyi0bPh",
        "outputId": "8bfe5d35-e536-45e1-8a96-f4853bcc8757"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPool2D, Dropout\n",
        "from tensorflow.keras import Model, Sequential\n",
        "\n",
        "import os\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
        "\n",
        "# print(tf.__version__)\n",
        "cifar10 = tf.keras.datasets.cifar10\n",
        "\n",
        "(X_train, Y_train), (X_test, Y_test) = cifar10.load_data()\n",
        "X_train, X_test = X_train / 255.0, X_test / 255.0\n",
        "\n",
        "print(Y_train[0:10])\n",
        "\n",
        "print(X_train.shape)\n",
        "\n",
        "# in the case of Keras or TF2, type shall be [image_size, image_size, 1]\n",
        "# X_train = X_train[..., tf.newaxis]\n",
        "# X_test = X_test[..., tf.newaxis]\n",
        "\n",
        "print(X_train.shape)\n",
        "\n",
        "batch_size = 1000\n",
        "# 입력된 buffer_size만큼 data를 채우고 무작위로 sampling하여 새로운 data로 바꿉니다.\n",
        "# 완벽한 셔플링을 위해서는 데이터 세트의 전체 크기보다 크거나 같은 버퍼 크기가 필요합니다.\n",
        "# 만약 작은 데이터수보다 작은 buffer_size를 사용할경우,\n",
        "# 처음에 설정된 buffer_size만큼의 data안에서 임의의 셔플링이 발생합니다.\n",
        "shuffle_size = 100000\n",
        "\n",
        "train_ds = tf.data.Dataset.from_tensor_slices(\n",
        "    (X_train, Y_train)).shuffle(shuffle_size).batch(batch_size)\n",
        "test_ds = tf.data.Dataset.from_tensor_slices((X_test, Y_test)).batch(batch_size)\n",
        "\n",
        "model = Sequential([\n",
        "    Conv2D(filters=64, kernel_size=3, activation=tf.nn.relu, padding='SAME',input_shape=(32, 32, 3)),\n",
        "    MaxPool2D(padding='SAME'),\n",
        "    Conv2D(filters=128, kernel_size=3, activation=tf.nn.relu, padding='SAME'),\n",
        "    MaxPool2D(padding='SAME'),\n",
        "    Conv2D(filters=256, kernel_size=3, activation=tf.nn.relu, padding='SAME'),\n",
        "    MaxPool2D(padding='SAME'),\n",
        "    Flatten(),\n",
        "    Dense(256, activation='relu'),\n",
        "    Dropout(0.2),\n",
        "    Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "model.summary()\n",
        "\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy()\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam()\n",
        "\n",
        "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
        "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')\n",
        "\n",
        "test_loss = tf.keras.metrics.Mean(name='test_loss')\n",
        "test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='test_accuracy')\n",
        "\n",
        "@tf.function\n",
        "def train_step(images, labels):\n",
        "    with tf.GradientTape() as tape:\n",
        "        predictions = model(images)\n",
        "        loss = loss_object(labels, predictions)\n",
        "    gradients = tape.gradient(loss, model.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "\n",
        "    train_loss(loss)\n",
        "    train_accuracy(labels, predictions)\n",
        "\n",
        "@tf.function\n",
        "def test_step(images, labels):\n",
        "    predictions = model(images)\n",
        "    t_loss = loss_object(labels, predictions)\n",
        "\n",
        "    test_loss(t_loss)\n",
        "    test_accuracy(labels, predictions)\n",
        "\n",
        "EPOCHS = 200\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    for images, labels in train_ds:\n",
        "        train_step(images, labels)\n",
        "\n",
        "    for test_images, test_labels in test_ds:\n",
        "        test_step(test_images, test_labels)\n",
        "\n",
        "    template = 'epoch: {:>5,d}, loss: {:>2.4f}, accuracy: {:>2.3f}%, test loss: {:>2.4f}, test accuracy: {:>2.3f}%'\n",
        "    print (template.format(epoch+1,\n",
        "                         train_loss.result(),\n",
        "                         train_accuracy.result()*100,\n",
        "                         test_loss.result(),\n",
        "                         test_accuracy.result()*100))\n",
        "\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[6]\n",
            " [9]\n",
            " [9]\n",
            " [4]\n",
            " [1]\n",
            " [1]\n",
            " [2]\n",
            " [7]\n",
            " [8]\n",
            " [3]]\n",
            "(50000, 32, 32, 3)\n",
            "(50000, 32, 32, 3)\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_3 (Conv2D)            (None, 32, 32, 64)        1792      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2 (None, 16, 16, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_4 (Conv2D)            (None, 16, 16, 128)       73856     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_4 (MaxPooling2 (None, 8, 8, 128)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_5 (Conv2D)            (None, 8, 8, 256)         295168    \n",
            "_________________________________________________________________\n",
            "max_pooling2d_5 (MaxPooling2 (None, 4, 4, 256)         0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 4096)              0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 256)               1048832   \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 10)                2570      \n",
            "=================================================================\n",
            "Total params: 1,422,218\n",
            "Trainable params: 1,422,218\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "epoch:     1, loss: 1.8446, accuracy: 33.678%, test loss: 1.5260, test accuracy: 44.390%\n",
            "epoch:     2, loss: 1.6310, accuracy: 41.472%, test loss: 1.4072, test accuracy: 49.010%\n",
            "epoch:     3, loss: 1.5010, accuracy: 46.388%, test loss: 1.3337, test accuracy: 51.973%\n",
            "epoch:     4, loss: 1.4033, accuracy: 50.000%, test loss: 1.2674, test accuracy: 54.688%\n",
            "epoch:     5, loss: 1.3236, accuracy: 53.013%, test loss: 1.2161, test accuracy: 56.774%\n",
            "epoch:     6, loss: 1.2583, accuracy: 55.472%, test loss: 1.1670, test accuracy: 58.705%\n",
            "epoch:     7, loss: 1.2007, accuracy: 57.606%, test loss: 1.1276, test accuracy: 60.219%\n",
            "epoch:     8, loss: 1.1512, accuracy: 59.429%, test loss: 1.0927, test accuracy: 61.545%\n",
            "epoch:     9, loss: 1.1080, accuracy: 61.026%, test loss: 1.0639, test accuracy: 62.591%\n",
            "epoch:    10, loss: 1.0694, accuracy: 62.452%, test loss: 1.0388, test accuracy: 63.527%\n",
            "epoch:    11, loss: 1.0334, accuracy: 63.772%, test loss: 1.0162, test accuracy: 64.383%\n",
            "epoch:    12, loss: 1.0020, accuracy: 64.933%, test loss: 1.0033, test accuracy: 64.930%\n",
            "epoch:    13, loss: 0.9720, accuracy: 66.031%, test loss: 0.9845, test accuracy: 65.644%\n",
            "epoch:    14, loss: 0.9432, accuracy: 67.084%, test loss: 0.9691, test accuracy: 66.251%\n",
            "epoch:    15, loss: 0.9167, accuracy: 68.037%, test loss: 0.9548, test accuracy: 66.815%\n",
            "epoch:    16, loss: 0.8908, accuracy: 68.974%, test loss: 0.9473, test accuracy: 67.219%\n",
            "epoch:    17, loss: 0.8667, accuracy: 69.836%, test loss: 0.9374, test accuracy: 67.655%\n",
            "epoch:    18, loss: 0.8435, accuracy: 70.662%, test loss: 0.9291, test accuracy: 68.056%\n",
            "epoch:    19, loss: 0.8211, accuracy: 71.464%, test loss: 0.9209, test accuracy: 68.416%\n",
            "epoch:    20, loss: 0.7995, accuracy: 72.240%, test loss: 0.9133, test accuracy: 68.792%\n",
            "epoch:    21, loss: 0.7785, accuracy: 72.990%, test loss: 0.9070, test accuracy: 69.116%\n",
            "epoch:    22, loss: 0.7587, accuracy: 73.691%, test loss: 0.9016, test accuracy: 69.395%\n",
            "epoch:    23, loss: 0.7391, accuracy: 74.391%, test loss: 0.8983, test accuracy: 69.641%\n",
            "epoch:    24, loss: 0.7194, accuracy: 75.091%, test loss: 0.8965, test accuracy: 69.842%\n",
            "epoch:    25, loss: 0.7010, accuracy: 75.751%, test loss: 0.8956, test accuracy: 70.030%\n",
            "epoch:    26, loss: 0.6827, accuracy: 76.402%, test loss: 0.8946, test accuracy: 70.233%\n",
            "epoch:    27, loss: 0.6649, accuracy: 77.042%, test loss: 0.8941, test accuracy: 70.441%\n",
            "epoch:    28, loss: 0.6475, accuracy: 77.657%, test loss: 0.8953, test accuracy: 70.586%\n",
            "epoch:    29, loss: 0.6308, accuracy: 78.256%, test loss: 0.8971, test accuracy: 70.759%\n",
            "epoch:    30, loss: 0.6143, accuracy: 78.847%, test loss: 0.9001, test accuracy: 70.892%\n",
            "epoch:    31, loss: 0.5983, accuracy: 79.417%, test loss: 0.9035, test accuracy: 71.039%\n",
            "epoch:    32, loss: 0.5832, accuracy: 79.957%, test loss: 0.9080, test accuracy: 71.153%\n",
            "epoch:    33, loss: 0.5681, accuracy: 80.501%, test loss: 0.9138, test accuracy: 71.285%\n",
            "epoch:    34, loss: 0.5536, accuracy: 81.018%, test loss: 0.9198, test accuracy: 71.406%\n",
            "epoch:    35, loss: 0.5397, accuracy: 81.513%, test loss: 0.9258, test accuracy: 71.531%\n",
            "epoch:    36, loss: 0.5262, accuracy: 81.995%, test loss: 0.9325, test accuracy: 71.654%\n",
            "epoch:    37, loss: 0.5131, accuracy: 82.459%, test loss: 0.9418, test accuracy: 71.736%\n",
            "epoch:    38, loss: 0.5005, accuracy: 82.905%, test loss: 0.9504, test accuracy: 71.838%\n",
            "epoch:    39, loss: 0.4884, accuracy: 83.335%, test loss: 0.9590, test accuracy: 71.937%\n",
            "epoch:    40, loss: 0.4767, accuracy: 83.747%, test loss: 0.9684, test accuracy: 72.040%\n",
            "epoch:    41, loss: 0.4654, accuracy: 84.142%, test loss: 0.9781, test accuracy: 72.142%\n",
            "epoch:    42, loss: 0.4546, accuracy: 84.518%, test loss: 0.9883, test accuracy: 72.233%\n",
            "epoch:    43, loss: 0.4443, accuracy: 84.877%, test loss: 0.9985, test accuracy: 72.316%\n",
            "epoch:    44, loss: 0.4344, accuracy: 85.220%, test loss: 1.0091, test accuracy: 72.390%\n",
            "epoch:    45, loss: 0.4249, accuracy: 85.548%, test loss: 1.0198, test accuracy: 72.473%\n",
            "epoch:    46, loss: 0.4158, accuracy: 85.862%, test loss: 1.0305, test accuracy: 72.551%\n",
            "epoch:    47, loss: 0.4070, accuracy: 86.163%, test loss: 1.0414, test accuracy: 72.619%\n",
            "epoch:    48, loss: 0.3986, accuracy: 86.451%, test loss: 1.0522, test accuracy: 72.686%\n",
            "epoch:    49, loss: 0.3906, accuracy: 86.727%, test loss: 1.0628, test accuracy: 72.754%\n",
            "epoch:    50, loss: 0.3828, accuracy: 86.993%, test loss: 1.0734, test accuracy: 72.818%\n",
            "epoch:    51, loss: 0.3754, accuracy: 87.248%, test loss: 1.0842, test accuracy: 72.879%\n",
            "epoch:    52, loss: 0.3682, accuracy: 87.493%, test loss: 1.0947, test accuracy: 72.940%\n",
            "epoch:    53, loss: 0.3613, accuracy: 87.729%, test loss: 1.1053, test accuracy: 72.998%\n",
            "epoch:    54, loss: 0.3546, accuracy: 87.956%, test loss: 1.1158, test accuracy: 73.054%\n",
            "epoch:    55, loss: 0.3482, accuracy: 88.175%, test loss: 1.1262, test accuracy: 73.109%\n",
            "epoch:    56, loss: 0.3420, accuracy: 88.386%, test loss: 1.1367, test accuracy: 73.160%\n",
            "epoch:    57, loss: 0.3361, accuracy: 88.590%, test loss: 1.1468, test accuracy: 73.212%\n",
            "epoch:    58, loss: 0.3303, accuracy: 88.787%, test loss: 1.1569, test accuracy: 73.261%\n",
            "epoch:    59, loss: 0.3247, accuracy: 88.977%, test loss: 1.1670, test accuracy: 73.309%\n",
            "epoch:    60, loss: 0.3193, accuracy: 89.160%, test loss: 1.1769, test accuracy: 73.354%\n",
            "epoch:    61, loss: 0.3141, accuracy: 89.338%, test loss: 1.1868, test accuracy: 73.396%\n",
            "epoch:    62, loss: 0.3091, accuracy: 89.510%, test loss: 1.1965, test accuracy: 73.440%\n",
            "epoch:    63, loss: 0.3042, accuracy: 89.677%, test loss: 1.2062, test accuracy: 73.484%\n",
            "epoch:    64, loss: 0.2994, accuracy: 89.838%, test loss: 1.2157, test accuracy: 73.524%\n",
            "epoch:    65, loss: 0.2948, accuracy: 89.994%, test loss: 1.2251, test accuracy: 73.563%\n",
            "epoch:    66, loss: 0.2904, accuracy: 90.146%, test loss: 1.2344, test accuracy: 73.602%\n",
            "epoch:    67, loss: 0.2861, accuracy: 90.293%, test loss: 1.2436, test accuracy: 73.638%\n",
            "epoch:    68, loss: 0.2819, accuracy: 90.436%, test loss: 1.2527, test accuracy: 73.674%\n",
            "epoch:    69, loss: 0.2778, accuracy: 90.574%, test loss: 1.2617, test accuracy: 73.708%\n",
            "epoch:    70, loss: 0.2738, accuracy: 90.709%, test loss: 1.2706, test accuracy: 73.740%\n",
            "epoch:    71, loss: 0.2700, accuracy: 90.840%, test loss: 1.2795, test accuracy: 73.774%\n",
            "epoch:    72, loss: 0.2663, accuracy: 90.967%, test loss: 1.2883, test accuracy: 73.805%\n",
            "epoch:    73, loss: 0.2626, accuracy: 91.091%, test loss: 1.2970, test accuracy: 73.836%\n",
            "epoch:    74, loss: 0.2591, accuracy: 91.211%, test loss: 1.3055, test accuracy: 73.866%\n",
            "epoch:    75, loss: 0.2556, accuracy: 91.328%, test loss: 1.3140, test accuracy: 73.895%\n",
            "epoch:    76, loss: 0.2523, accuracy: 91.443%, test loss: 1.3224, test accuracy: 73.924%\n",
            "epoch:    77, loss: 0.2490, accuracy: 91.554%, test loss: 1.3308, test accuracy: 73.951%\n",
            "epoch:    78, loss: 0.2458, accuracy: 91.662%, test loss: 1.3390, test accuracy: 73.978%\n",
            "epoch:    79, loss: 0.2427, accuracy: 91.767%, test loss: 1.3472, test accuracy: 74.001%\n",
            "epoch:    80, loss: 0.2397, accuracy: 91.870%, test loss: 1.3552, test accuracy: 74.027%\n",
            "epoch:    81, loss: 0.2367, accuracy: 91.971%, test loss: 1.3633, test accuracy: 74.051%\n",
            "epoch:    82, loss: 0.2338, accuracy: 92.069%, test loss: 1.3712, test accuracy: 74.075%\n",
            "epoch:    83, loss: 0.2310, accuracy: 92.164%, test loss: 1.3790, test accuracy: 74.099%\n",
            "epoch:    84, loss: 0.2283, accuracy: 92.257%, test loss: 1.3868, test accuracy: 74.121%\n",
            "epoch:    85, loss: 0.2256, accuracy: 92.349%, test loss: 1.3946, test accuracy: 74.143%\n",
            "epoch:    86, loss: 0.2230, accuracy: 92.438%, test loss: 1.4022, test accuracy: 74.165%\n",
            "epoch:    87, loss: 0.2204, accuracy: 92.524%, test loss: 1.4098, test accuracy: 74.184%\n",
            "epoch:    88, loss: 0.2179, accuracy: 92.609%, test loss: 1.4173, test accuracy: 74.204%\n",
            "epoch:    89, loss: 0.2155, accuracy: 92.692%, test loss: 1.4248, test accuracy: 74.225%\n",
            "epoch:    90, loss: 0.2131, accuracy: 92.774%, test loss: 1.4322, test accuracy: 74.243%\n",
            "epoch:    91, loss: 0.2108, accuracy: 92.853%, test loss: 1.4395, test accuracy: 74.262%\n",
            "epoch:    92, loss: 0.2085, accuracy: 92.931%, test loss: 1.4468, test accuracy: 74.281%\n",
            "epoch:    93, loss: 0.2062, accuracy: 93.007%, test loss: 1.4540, test accuracy: 74.299%\n",
            "epoch:    94, loss: 0.2040, accuracy: 93.081%, test loss: 1.4612, test accuracy: 74.316%\n",
            "epoch:    95, loss: 0.2019, accuracy: 93.154%, test loss: 1.4683, test accuracy: 74.332%\n",
            "epoch:    96, loss: 0.1998, accuracy: 93.225%, test loss: 1.4754, test accuracy: 74.349%\n",
            "epoch:    97, loss: 0.1977, accuracy: 93.295%, test loss: 1.4824, test accuracy: 74.366%\n",
            "epoch:    98, loss: 0.1957, accuracy: 93.364%, test loss: 1.4893, test accuracy: 74.382%\n",
            "epoch:    99, loss: 0.1937, accuracy: 93.431%, test loss: 1.4962, test accuracy: 74.398%\n",
            "epoch:   100, loss: 0.1918, accuracy: 93.496%, test loss: 1.5030, test accuracy: 74.414%\n",
            "epoch:   101, loss: 0.1899, accuracy: 93.561%, test loss: 1.5098, test accuracy: 74.429%\n",
            "epoch:   102, loss: 0.1881, accuracy: 93.624%, test loss: 1.5166, test accuracy: 74.443%\n",
            "epoch:   103, loss: 0.1862, accuracy: 93.686%, test loss: 1.5233, test accuracy: 74.457%\n",
            "epoch:   104, loss: 0.1844, accuracy: 93.746%, test loss: 1.5299, test accuracy: 74.471%\n",
            "epoch:   105, loss: 0.1827, accuracy: 93.806%, test loss: 1.5365, test accuracy: 74.485%\n",
            "epoch:   106, loss: 0.1810, accuracy: 93.864%, test loss: 1.5430, test accuracy: 74.497%\n",
            "epoch:   107, loss: 0.1793, accuracy: 93.922%, test loss: 1.5495, test accuracy: 74.511%\n",
            "epoch:   108, loss: 0.1776, accuracy: 93.978%, test loss: 1.5560, test accuracy: 74.524%\n",
            "epoch:   109, loss: 0.1760, accuracy: 94.033%, test loss: 1.5624, test accuracy: 74.537%\n",
            "epoch:   110, loss: 0.1744, accuracy: 94.088%, test loss: 1.5688, test accuracy: 74.549%\n",
            "epoch:   111, loss: 0.1728, accuracy: 94.141%, test loss: 1.5752, test accuracy: 74.560%\n",
            "epoch:   112, loss: 0.1713, accuracy: 94.193%, test loss: 1.5814, test accuracy: 74.572%\n",
            "epoch:   113, loss: 0.1698, accuracy: 94.245%, test loss: 1.5877, test accuracy: 74.583%\n",
            "epoch:   114, loss: 0.1683, accuracy: 94.295%, test loss: 1.5939, test accuracy: 74.594%\n",
            "epoch:   115, loss: 0.1668, accuracy: 94.345%, test loss: 1.6000, test accuracy: 74.606%\n",
            "epoch:   116, loss: 0.1654, accuracy: 94.393%, test loss: 1.6062, test accuracy: 74.617%\n",
            "epoch:   117, loss: 0.1640, accuracy: 94.441%, test loss: 1.6123, test accuracy: 74.627%\n",
            "epoch:   118, loss: 0.1626, accuracy: 94.488%, test loss: 1.6184, test accuracy: 74.637%\n",
            "epoch:   119, loss: 0.1612, accuracy: 94.535%, test loss: 1.6244, test accuracy: 74.648%\n",
            "epoch:   120, loss: 0.1599, accuracy: 94.580%, test loss: 1.6304, test accuracy: 74.658%\n",
            "epoch:   121, loss: 0.1585, accuracy: 94.625%, test loss: 1.6364, test accuracy: 74.667%\n",
            "epoch:   122, loss: 0.1572, accuracy: 94.669%, test loss: 1.6423, test accuracy: 74.677%\n",
            "epoch:   123, loss: 0.1560, accuracy: 94.712%, test loss: 1.6482, test accuracy: 74.686%\n",
            "epoch:   124, loss: 0.1547, accuracy: 94.755%, test loss: 1.6541, test accuracy: 74.696%\n",
            "epoch:   125, loss: 0.1535, accuracy: 94.797%, test loss: 1.6600, test accuracy: 74.704%\n",
            "epoch:   126, loss: 0.1523, accuracy: 94.838%, test loss: 1.6658, test accuracy: 74.713%\n",
            "epoch:   127, loss: 0.1511, accuracy: 94.879%, test loss: 1.6716, test accuracy: 74.721%\n",
            "epoch:   128, loss: 0.1499, accuracy: 94.919%, test loss: 1.6773, test accuracy: 74.730%\n",
            "epoch:   129, loss: 0.1487, accuracy: 94.958%, test loss: 1.6830, test accuracy: 74.738%\n",
            "epoch:   130, loss: 0.1476, accuracy: 94.997%, test loss: 1.6887, test accuracy: 74.746%\n",
            "epoch:   131, loss: 0.1464, accuracy: 95.035%, test loss: 1.6944, test accuracy: 74.756%\n",
            "epoch:   132, loss: 0.1453, accuracy: 95.073%, test loss: 1.7000, test accuracy: 74.763%\n",
            "epoch:   133, loss: 0.1442, accuracy: 95.110%, test loss: 1.7056, test accuracy: 74.771%\n",
            "epoch:   134, loss: 0.1432, accuracy: 95.146%, test loss: 1.7112, test accuracy: 74.778%\n",
            "epoch:   135, loss: 0.1421, accuracy: 95.182%, test loss: 1.7167, test accuracy: 74.786%\n",
            "epoch:   136, loss: 0.1411, accuracy: 95.218%, test loss: 1.7223, test accuracy: 74.793%\n",
            "epoch:   137, loss: 0.1400, accuracy: 95.253%, test loss: 1.7278, test accuracy: 74.801%\n",
            "epoch:   138, loss: 0.1390, accuracy: 95.287%, test loss: 1.7332, test accuracy: 74.807%\n",
            "epoch:   139, loss: 0.1380, accuracy: 95.321%, test loss: 1.7387, test accuracy: 74.815%\n",
            "epoch:   140, loss: 0.1370, accuracy: 95.354%, test loss: 1.7441, test accuracy: 74.821%\n",
            "epoch:   141, loss: 0.1361, accuracy: 95.387%, test loss: 1.7495, test accuracy: 74.828%\n",
            "epoch:   142, loss: 0.1351, accuracy: 95.420%, test loss: 1.7549, test accuracy: 74.835%\n",
            "epoch:   143, loss: 0.1342, accuracy: 95.452%, test loss: 1.7603, test accuracy: 74.842%\n",
            "epoch:   144, loss: 0.1332, accuracy: 95.484%, test loss: 1.7656, test accuracy: 74.849%\n",
            "epoch:   145, loss: 0.1323, accuracy: 95.515%, test loss: 1.7709, test accuracy: 74.855%\n",
            "epoch:   146, loss: 0.1314, accuracy: 95.545%, test loss: 1.7762, test accuracy: 74.862%\n",
            "epoch:   147, loss: 0.1305, accuracy: 95.576%, test loss: 1.7815, test accuracy: 74.868%\n",
            "epoch:   148, loss: 0.1296, accuracy: 95.606%, test loss: 1.7867, test accuracy: 74.873%\n",
            "epoch:   149, loss: 0.1288, accuracy: 95.635%, test loss: 1.7920, test accuracy: 74.879%\n",
            "epoch:   150, loss: 0.1279, accuracy: 95.664%, test loss: 1.7972, test accuracy: 74.885%\n",
            "epoch:   151, loss: 0.1271, accuracy: 95.693%, test loss: 1.8024, test accuracy: 74.890%\n",
            "epoch:   152, loss: 0.1262, accuracy: 95.721%, test loss: 1.8075, test accuracy: 74.896%\n",
            "epoch:   153, loss: 0.1254, accuracy: 95.749%, test loss: 1.8127, test accuracy: 74.902%\n",
            "epoch:   154, loss: 0.1246, accuracy: 95.777%, test loss: 1.8178, test accuracy: 74.907%\n",
            "epoch:   155, loss: 0.1238, accuracy: 95.804%, test loss: 1.8229, test accuracy: 74.913%\n",
            "epoch:   156, loss: 0.1230, accuracy: 95.831%, test loss: 1.8280, test accuracy: 74.917%\n",
            "epoch:   157, loss: 0.1222, accuracy: 95.858%, test loss: 1.8330, test accuracy: 74.922%\n",
            "epoch:   158, loss: 0.1214, accuracy: 95.884%, test loss: 1.8381, test accuracy: 74.927%\n",
            "epoch:   159, loss: 0.1207, accuracy: 95.910%, test loss: 1.8431, test accuracy: 74.933%\n",
            "epoch:   160, loss: 0.1199, accuracy: 95.935%, test loss: 1.8481, test accuracy: 74.938%\n",
            "epoch:   161, loss: 0.1192, accuracy: 95.960%, test loss: 1.8531, test accuracy: 74.942%\n",
            "epoch:   162, loss: 0.1184, accuracy: 95.985%, test loss: 1.8581, test accuracy: 74.947%\n",
            "epoch:   163, loss: 0.1177, accuracy: 96.010%, test loss: 1.8631, test accuracy: 74.952%\n",
            "epoch:   164, loss: 0.1170, accuracy: 96.034%, test loss: 1.8680, test accuracy: 74.957%\n",
            "epoch:   165, loss: 0.1163, accuracy: 96.058%, test loss: 1.8729, test accuracy: 74.961%\n",
            "epoch:   166, loss: 0.1156, accuracy: 96.082%, test loss: 1.8779, test accuracy: 74.966%\n",
            "epoch:   167, loss: 0.1149, accuracy: 96.106%, test loss: 1.8827, test accuracy: 74.970%\n",
            "epoch:   168, loss: 0.1142, accuracy: 96.129%, test loss: 1.8876, test accuracy: 74.975%\n",
            "epoch:   169, loss: 0.1135, accuracy: 96.152%, test loss: 1.8925, test accuracy: 74.979%\n",
            "epoch:   170, loss: 0.1129, accuracy: 96.174%, test loss: 1.8973, test accuracy: 74.984%\n",
            "epoch:   171, loss: 0.1122, accuracy: 96.197%, test loss: 1.9022, test accuracy: 74.988%\n",
            "epoch:   172, loss: 0.1115, accuracy: 96.219%, test loss: 1.9070, test accuracy: 74.992%\n",
            "epoch:   173, loss: 0.1109, accuracy: 96.241%, test loss: 1.9118, test accuracy: 74.996%\n",
            "epoch:   174, loss: 0.1103, accuracy: 96.262%, test loss: 1.9166, test accuracy: 75.001%\n",
            "epoch:   175, loss: 0.1096, accuracy: 96.284%, test loss: 1.9213, test accuracy: 75.005%\n",
            "epoch:   176, loss: 0.1090, accuracy: 96.305%, test loss: 1.9261, test accuracy: 75.009%\n",
            "epoch:   177, loss: 0.1084, accuracy: 96.326%, test loss: 1.9308, test accuracy: 75.013%\n",
            "epoch:   178, loss: 0.1078, accuracy: 96.346%, test loss: 1.9356, test accuracy: 75.017%\n",
            "epoch:   179, loss: 0.1072, accuracy: 96.367%, test loss: 1.9403, test accuracy: 75.021%\n",
            "epoch:   180, loss: 0.1066, accuracy: 96.387%, test loss: 1.9450, test accuracy: 75.025%\n",
            "epoch:   181, loss: 0.1060, accuracy: 96.407%, test loss: 1.9497, test accuracy: 75.029%\n",
            "epoch:   182, loss: 0.1054, accuracy: 96.427%, test loss: 1.9543, test accuracy: 75.033%\n",
            "epoch:   183, loss: 0.1048, accuracy: 96.446%, test loss: 1.9590, test accuracy: 75.037%\n",
            "epoch:   184, loss: 0.1043, accuracy: 96.465%, test loss: 1.9636, test accuracy: 75.040%\n",
            "epoch:   185, loss: 0.1037, accuracy: 96.484%, test loss: 1.9683, test accuracy: 75.045%\n",
            "epoch:   186, loss: 0.1031, accuracy: 96.503%, test loss: 1.9729, test accuracy: 75.048%\n",
            "epoch:   187, loss: 0.1026, accuracy: 96.522%, test loss: 1.9775, test accuracy: 75.052%\n",
            "epoch:   188, loss: 0.1021, accuracy: 96.541%, test loss: 1.9821, test accuracy: 75.056%\n",
            "epoch:   189, loss: 0.1015, accuracy: 96.559%, test loss: 1.9867, test accuracy: 75.060%\n",
            "epoch:   190, loss: 0.1010, accuracy: 96.577%, test loss: 1.9913, test accuracy: 75.063%\n",
            "epoch:   191, loss: 0.1004, accuracy: 96.595%, test loss: 1.9959, test accuracy: 75.067%\n",
            "epoch:   192, loss: 0.0999, accuracy: 96.613%, test loss: 2.0004, test accuracy: 75.070%\n",
            "epoch:   193, loss: 0.0994, accuracy: 96.630%, test loss: 2.0049, test accuracy: 75.073%\n",
            "epoch:   194, loss: 0.0989, accuracy: 96.648%, test loss: 2.0095, test accuracy: 75.077%\n",
            "epoch:   195, loss: 0.0984, accuracy: 96.665%, test loss: 2.0140, test accuracy: 75.080%\n",
            "epoch:   196, loss: 0.0979, accuracy: 96.682%, test loss: 2.0185, test accuracy: 75.083%\n",
            "epoch:   197, loss: 0.0974, accuracy: 96.699%, test loss: 2.0230, test accuracy: 75.087%\n",
            "epoch:   198, loss: 0.0969, accuracy: 96.715%, test loss: 2.0275, test accuracy: 75.090%\n",
            "epoch:   199, loss: 0.0964, accuracy: 96.732%, test loss: 2.0320, test accuracy: 75.093%\n",
            "epoch:   200, loss: 0.0959, accuracy: 96.748%, test loss: 2.0364, test accuracy: 75.097%\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}