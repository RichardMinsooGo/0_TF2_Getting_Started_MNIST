{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "21_TF2_MNIST_expert_sequential_sparse.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PmcPzSWs-LTI"
      },
      "source": [
        "# MNIST\n",
        "\n",
        "아래의 code는 google colab 에서 시험된 결과이다.\n",
        "Anaconda 에서 제공되는 jupyter notebook에서는 작동이 안될수 있다.\n",
        "추천하는 방법은 notebook이 아니라 python 파일 자체를 실행하라는 것이다. TPU 예제만 빼고는 anaconda prompt 에서 *.py 파일을 실행하는 방법을 추천한다.\n",
        "compare 도구로서는 \"BCcompare\"를 추천한다. working day로 30일간 무료로 사용이 가능하다.\n",
        "무료 사용후 재 설치 하면 다시 30 working day동안 사용이 가능하다.\n",
        "\n",
        "Compare 도구를 사용하여 \"11_TF2_MNIST_sparse_network_modify.py\"와 \"21_TF2_MNIST_expert_sequential_sparse.py\"를 비교하여 보면 \"tf.data.Dataset.from_tensor_slices\"가 추가가 되어진것을 볼수 있다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DSrU2CExdfzU"
      },
      "source": [
        "\n",
        "```\n",
        "batch_size = 1000\n",
        "# 입력된 buffer_size만큼 data를 채우고 무작위로 sampling하여 새로운 data로 바꿉니다.\n",
        "# 완벽한 셔플링을 위해서는 데이터 세트의 전체 크기보다 크거나 같은 버퍼 크기가 필요합니다.\n",
        "# 만약 작은 데이터수보다 작은 buffer_size를 사용할경우,\n",
        "# 처음에 설정된 buffer_size만큼의 data안에서 임의의 셔플링이 발생합니다.\n",
        "shuffle_size = 100000\n",
        "\n",
        "train_ds = tf.data.Dataset.from_tensor_slices(\n",
        "    (X_train, Y_train)).shuffle(shuffle_size).batch(batch_size)\n",
        "test_ds = tf.data.Dataset.from_tensor_slices((X_test, Y_test)).batch(batch_size)\n",
        "\n",
        "```\n",
        "\n",
        "\"tf.data.Dataset.\"의 사용법에 대한 자세한 강의는 다른 부분에서 진행할 예정입니다.\n",
        "이를 사용하는 주된 목적은 deel learning machine의 resource 때문으로 보면 됩니다.\n",
        "\n",
        "Tensorflow에서 제공하는 MNIST나 cifar10, cifar100를 일반 laptop의 memory 에서 실행이 가능합니다.\n",
        "하지만 image의 사이즈가 224 pixel로만 커져도 감당이 되지 않고, data를 쪼개서 학습을 시켜야 합니다.\n",
        "이 저장소에서는 이러한것이 있다는 것 정도만 알면 됩니다.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jd40kKhmda4R",
        "outputId": "186739c2-274f-4209-b7aa-4287f54f1a2f"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPool2D, Dropout\n",
        "from tensorflow.keras import Model, Sequential\n",
        "\n",
        "import os\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
        "\n",
        "# print(tf.__version__)\n",
        "## MNIST Dataset #########################################################\n",
        "mnist = tf.keras.datasets.mnist\n",
        "# class_names = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n",
        "##########################################################################\n",
        "\n",
        "## Fashion MNIST Dataset #################################################\n",
        "# mnist = tf.keras.datasets.fashion_mnist\n",
        "# class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
        "##########################################################################\n",
        "(X_train, Y_train), (X_test, Y_test) = mnist.load_data()    \n",
        "\n",
        "# Change data type as float. If it is unt type, it might cause error \n",
        "X_train, X_test = X_train / 255.0, X_test / 255.0\n",
        "\n",
        "print(Y_train[0:10])\n",
        "\n",
        "print(X_train.shape)\n",
        "\n",
        "# in the case of Keras or TF2, type shall be [image_size, image_size, 1]\n",
        "# if it is RGB type, type shall be [image_size, image_size, 3]\n",
        "# For MNIST or Fashion MNIST, it need to reshape\n",
        "X_train = X_train[..., tf.newaxis]\n",
        "X_test = X_test[..., tf.newaxis]\n",
        "\n",
        "print(X_train.shape)\n",
        "\n",
        "batch_size = 1000\n",
        "# 입력된 buffer_size만큼 data를 채우고 무작위로 sampling하여 새로운 data로 바꿉니다.\n",
        "# 완벽한 셔플링을 위해서는 데이터 세트의 전체 크기보다 크거나 같은 버퍼 크기가 필요합니다.\n",
        "# 만약 작은 데이터수보다 작은 buffer_size를 사용할경우,\n",
        "# 처음에 설정된 buffer_size만큼의 data안에서 임의의 셔플링이 발생합니다.\n",
        "shuffle_size = 100000\n",
        "\n",
        "train_ds = tf.data.Dataset.from_tensor_slices(\n",
        "    (X_train, Y_train)).shuffle(shuffle_size).batch(batch_size)\n",
        "test_ds = tf.data.Dataset.from_tensor_slices((X_test, Y_test)).batch(batch_size)\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n",
            "[5 0 4 1 9 2 1 3 1 4]\n",
            "(60000, 28, 28)\n",
            "(60000, 28, 28, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zVxhwRMWkrR_"
      },
      "source": [
        "아래의 code 부분은 초보자 모드와 동일하다.\n",
        "네트워크를 구성하는 방법은 바뀔필요는 없다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xryjQ-IUk8v9",
        "outputId": "6312516f-fc15-499e-efd3-c02730ce65a6"
      },
      "source": [
        "\n",
        "model = Sequential([\n",
        "    Conv2D(filters=64, kernel_size=3, activation=tf.nn.relu, padding='SAME',input_shape=(28, 28, 1)),\n",
        "    MaxPool2D(padding='SAME'),\n",
        "    Conv2D(filters=128, kernel_size=3, activation=tf.nn.relu, padding='SAME'),\n",
        "    MaxPool2D(padding='SAME'),\n",
        "    Conv2D(filters=256, kernel_size=3, activation=tf.nn.relu, padding='SAME'),\n",
        "    MaxPool2D(padding='SAME'),\n",
        "    Flatten(),\n",
        "    Dense(256, activation='relu'),\n",
        "    Dropout(0.2),\n",
        "    Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "model.summary()\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d (Conv2D)              (None, 28, 28, 64)        640       \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 14, 14, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 14, 14, 128)       73856     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 7, 7, 128)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 7, 7, 256)         295168    \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 4, 4, 256)         0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 4096)              0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 256)               1048832   \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 10)                2570      \n",
            "=================================================================\n",
            "Total params: 1,421,066\n",
            "Trainable params: 1,421,066\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tIk8UQQOrnlC"
      },
      "source": [
        "Compare 도구를 사용하여 \"11_TF2_MNIST_sparse_network_modify.py\"와 \"21_TF2_MNIST_expert_sequential_sparse.py\"를 비교하여 보면, 전문가 모드에서는 \"tf.GradientTape()\"가 사용되져서 구성되어져 있다는것을 알수 있다.\n",
        "\n",
        "아래의 코드가 복잡하게 보이는 이유는 \"tf.data.Dataset.\"와 \"tf.GradientTape()\"이 동시에 사용되어 졌기 때문이다.\n",
        "\n",
        "\"tf.GradientTape()\"가 필요한 시점이 오면 이를 자세히 다루도록 하겠다.\n",
        "아래의 code를 향후에 복잡한 code들을 구성할때에 약간씩만 고쳐서 사용하도록 할 예정이므로, 아래를 통째로 가지고 다니도록 하자."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sYXK53tcsosj",
        "outputId": "ec770915-dffb-4277-81e4-40ce09b93f1a"
      },
      "source": [
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy()\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam()\n",
        "\n",
        "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
        "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')\n",
        "\n",
        "test_loss = tf.keras.metrics.Mean(name='test_loss')\n",
        "test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='test_accuracy')\n",
        "\n",
        "@tf.function\n",
        "def train_step(images, labels):\n",
        "    with tf.GradientTape() as tape:\n",
        "        predictions = model(images)\n",
        "        loss = loss_object(labels, predictions)\n",
        "    gradients = tape.gradient(loss, model.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "\n",
        "    train_loss(loss)\n",
        "    train_accuracy(labels, predictions)\n",
        "\n",
        "@tf.function\n",
        "def test_step(images, labels):\n",
        "    predictions = model(images)\n",
        "    t_loss = loss_object(labels, predictions)\n",
        "\n",
        "    test_loss(t_loss)\n",
        "    test_accuracy(labels, predictions)\n",
        "\n",
        "EPOCHS = 5\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    for images, labels in train_ds:\n",
        "        train_step(images, labels)\n",
        "\n",
        "    for test_images, test_labels in test_ds:\n",
        "        test_step(test_images, test_labels)\n",
        "\n",
        "    template = 'epoch: {:>5,d}, loss: {:>2.4f}, accuracy: {:>2.3f}%, test loss: {:>2.4f}, test accuracy: {:>2.3f}%'\n",
        "    print (template.format(epoch+1,\n",
        "                         train_loss.result(),\n",
        "                         train_accuracy.result()*100,\n",
        "                         test_loss.result(),\n",
        "                         test_accuracy.result()*100))           \n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch:     1, loss: 0.5213, accuracy: 83.848%, test loss: 0.1058, test accuracy: 96.390%\n",
            "epoch:     2, loss: 0.3012, accuracy: 90.673%, test loss: 0.0774, test accuracy: 97.420%\n",
            "epoch:     3, loss: 0.2171, accuracy: 93.277%, test loss: 0.0623, test accuracy: 97.913%\n",
            "epoch:     4, loss: 0.1722, accuracy: 94.661%, test loss: 0.0560, test accuracy: 98.145%\n",
            "epoch:     5, loss: 0.1442, accuracy: 95.528%, test loss: 0.0510, test accuracy: 98.302%\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}